{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "660ce795-9307-4c2c-98a1-beabcb36c740",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/module-0/basics.ipynb) [![Open in LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/56295530-getting-set-up-video-guide)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef597741-3211-4ecc-92f7-f58023ee237e",
   "metadata": {},
   "source": [
    "# LangChain Academy\n",
    "\n",
    "Welcome to LangChain Academy! \n",
    "\n",
    "## Context\n",
    "\n",
    "At LangChain, we aim to make it easy to build LLM applications. One type of LLM application you can build is an agent. There‚Äôs a lot of excitement around building agents because they can automate a wide range of tasks that were previously impossible. \n",
    "\n",
    "In practice though, it is incredibly difficult to build systems that reliably execute on these tasks. As we‚Äôve worked with our users to put agents into production, we‚Äôve learned that more control is often necessary. You might need an agent to always call a specific tool first or use different prompts based on its state. \n",
    "\n",
    "To tackle this problem, we‚Äôve built [LangGraph](https://langchain-ai.github.io/langgraph/) ‚Äî a framework for building agent and multi-agent applications. Separate from the LangChain package, LangGraph‚Äôs core design philosophy is to help developers add better precision and control into agent workflows, suitable for the complexity of real-world systems.\n",
    "\n",
    "## Course Structure\n",
    "\n",
    "The course is structured as a set of modules, with each module focused on a particular theme related to LangGraph. You will see a folder for each module, which contains a series of notebooks. A video will accompany each notebook to help walk through the concepts, but the notebooks are also stand-alone, meaning that they contain explanations and can be viewed independently of the videos. Each module folder also contains a `studio` folder, which contains a set of graphs that can be loaded into [LangGraph Studio](https://github.com/langchain-ai/langgraph-studio), our IDE for building LangGraph applications.\n",
    "\n",
    "## Setup\n",
    "\n",
    "Before you begin, please follow the instructions in the `README` to create an environment and install dependencies.\n",
    "\n",
    "## Chat models\n",
    "\n",
    "In this course, we'll be using [Chat Models](https://python.langchain.com/v0.2/docs/concepts/#chat-models), which do a few things take a sequence of messages as inputs and return chat messages as outputs. LangChain does not host any Chat Models, rather we rely on third party integrations. [Here](https://python.langchain.com/v0.2/docs/integrations/chat/) is a list of 3rd party chat model integrations within LangChain! By default, the course will use [ChatOpenAI](https://python.langchain.com/v0.2/docs/integrations/chat/openai/) because it is both popular and performant. As noted, please ensure that you have an `OPENAI_API_KEY`.\n",
    "\n",
    "Let's check that your `OPENAI_API_KEY` is set and, if not, you will be asked to enter it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f9a52c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U langchain_openai langchain_core langchain_community tavily-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2a15227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "GROQ_API_KEY:  ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
     ]
    }
   ],
   "source": [
    "import os, getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a326f35b",
   "metadata": {},
   "source": [
    "[Here](https://python.langchain.com/v0.2/docs/how_to/#chat-models) is a useful how-to for all the things that you can do with chat models, but we'll show a few highlights below. If you've run `pip install -r requirements.txt` as noted in the README, then you've installed the `langchain-openai` package. With this, we can instantiate our `ChatOpenAI` model object. If you are signing up for the API for the first time, you should receive [free credits](https://community.openai.com/t/understanding-api-limits-and-free-tier/498517) that can be applied to any of the models. You can see pricing for various models [here](https://openai.com/api/pricing/). The notebooks will default to `gpt-4o` because it's a good balance of quality, price, and speed [see more here](https://help.openai.com/en/articles/7102672-how-can-i-access-gpt-4-gpt-4-turbo-gpt-4o-and-gpt-4o-mini), but you can also opt for the lower priced `gpt-3.5` series models. \n",
    "\n",
    "There are [a few standard parameters](https://python.langchain.com/v0.2/docs/concepts/#chat-models) that we can set with chat models. Two of the most common are:\n",
    "\n",
    "* `model`: the name of the model\n",
    "* `temperature`: the sampling temperature\n",
    "\n",
    "`Temperature` controls the randomness or creativity of the model's output where low temperature (close to 0) is more deterministic and focused outputs. This is good for tasks requiring accuracy or factual responses. High temperature (close to 1) is good for creative tasks or generating varied responses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e19a54d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "gemma2_9b_chat = ChatGroq(model=\"gemma2-9b-it\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28450d1b",
   "metadata": {},
   "source": [
    "Chat models in LangChain have a number of [default methods](https://python.langchain.com/v0.2/docs/concepts/#runnable-interface). For the most part, we'll be using:\n",
    "\n",
    "* `stream`: stream back chunks of the response\n",
    "* `invoke`: call the chain on an input\n",
    "\n",
    "And, as mentioned, chat models take [messages](https://python.langchain.com/v0.2/docs/concepts/#messages) as input. Messages have a role (that describes who is saying the message) and a content property. We'll be talking a lot more about this later, but here let's just show the basics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1280e1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello world! üëã \\n\\nHow can I help you today?\\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 11, 'total_tokens': 27, 'completion_time': 0.029090909, 'prompt_time': 0.001893477, 'queue_time': 0.16741315199999998, 'total_time': 0.030984386}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-503515f7-f0b5-434c-93c9-e9cede2534a3-0', usage_metadata={'input_tokens': 11, 'output_tokens': 16, 'total_tokens': 27})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Create a message\n",
    "msg = HumanMessage(content=\"Hello world\", name=\"Lance\")\n",
    "\n",
    "# Message list\n",
    "messages = [msg]\n",
    "\n",
    "# Invoke the model with a list of messages \n",
    "gemma2_9b_chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac73e4c",
   "metadata": {},
   "source": [
    "We get an `AIMessage` response. Also, note that we can just invoke a chat model with a string. When a string is passed in as input, it is converted to a `HumanMessage` and then passed to the underlying model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f27c6c9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello world! üëã \\n\\nIs there anything else I can help you with?\\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 11, 'total_tokens': 30, 'completion_time': 0.034545455, 'prompt_time': 0.001911516, 'queue_time': 0.166746982, 'total_time': 0.036456971}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-89bc4fba-c31b-41c3-ba84-8e42d3253842-0', usage_metadata={'input_tokens': 11, 'output_tokens': 19, 'total_tokens': 30})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemma2_9b_chat.invoke(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "723ff3d6-22f0-4847-8d5c-77666a35af87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='–ê–ª–µ–∫—Å–∞–Ω–¥—Ä –°–µ—Ä–≥–µ–µ–≤–∏—á –ü—É—à–∫–∏–Ω (1799-1837) - **–≤–µ–ª–∏—á–∞–π—à–∏–π —Ä—É—Å—Å–∫–∏–π –ø–æ—ç—Ç –∏ –ø–∏—Å–∞—Ç–µ–ª—å**, –ø—Ä–∏–∑–Ω–∞–Ω–Ω—ã–π –æ—Å–Ω–æ–≤–æ–ø–æ–ª–æ–∂–Ω–∏–∫–æ–º —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–π —Ä—É—Å—Å–∫–æ–π –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã. \\n\\n**–û—Å–Ω–æ–≤–Ω—ã–µ —Ñ–∞–∫—Ç—ã –æ –ü—É—à–∫–∏–Ω–µ:**\\n\\n* **–†–æ–¥–∏–ª—Å—è** –≤ –ú–æ—Å–∫–≤–µ –≤ –¥–≤–æ—Ä—è–Ω—Å–∫–æ–π —Å–µ–º—å–µ.\\n* **–ü–æ–ª—É—á–∏–ª** –ø—Ä–µ–∫—Ä–∞—Å–Ω–æ–µ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ, –∏–∑—É—á–∞–ª –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—É, –∏—Å—Ç–æ—Ä–∏—é –∏ —è–∑—ã–∫–∏.\\n* **–ù–∞—á–∞–ª** –ø–∏—Å–∞—Ç—å —Å—Ç–∏—Ö–∏ –≤ —é–Ω–æ–º –≤–æ–∑—Ä–∞—Å—Ç–µ.\\n* **–ü–µ—Ä–≤—ã–µ –∏–∑–≤–µ—Å—Ç–Ω—ã–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è:** \"–†—É—Å–ª–∞–Ω –∏ –õ—é–¥–º–∏–ª–∞\", \"–ö–∞–ø–∏—Ç–∞–Ω—Å–∫–∞—è –¥–æ—á–∫–∞\", \"–ï–≤–≥–µ–Ω–∏–π –û–Ω–µ–≥–∏–Ω\".\\n* **–ë—ã–ª** —É—á–∞—Å—Ç–Ω–∏–∫–æ–º –¥–µ–∫–∞–±—Ä–∏—Å—Ç—Å–∫–æ–≥–æ –¥–≤–∏–∂–µ–Ω–∏—è, —á—Ç–æ –ø—Ä–∏–≤–µ–ª–æ –∫ –µ–≥–æ —Å—Å—ã–ª–∫–µ –≤ —é–∂–Ω—É—é —á–∞—Å—Ç—å –†–æ—Å—Å–∏–∏.\\n* **–°–∫–æ–Ω—á–∞–ª—Å—è** –≤ –≤–æ–∑—Ä–∞—Å—Ç–µ 37 –ª–µ—Ç –æ—Ç —Ä–∞–Ω, –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –Ω–∞ –¥—É—ç–ª–∏.\\n\\n**–ü—É—à–∫–∏–Ω - –∞–≤—Ç–æ—Ä:**\\n\\n* **–ü–æ—ç–º—ã:** \"–†—É—Å–ª–∞–Ω –∏ –õ—é–¥–º–∏–ª–∞\", \"–ü–æ–ª—Ç–∞–≤–∞\", \"–¶—ã–≥–∞–Ω—ã\"\\n* **–†–æ–º–∞–Ω–∞ –≤ —Å—Ç–∏—Ö–∞—Ö:** \"–ï–≤–≥–µ–Ω–∏–π –û–Ω–µ–≥–∏–Ω\"\\n* **–î—Ä–∞–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–π:** \"–ë–æ—Ä–∏—Å –ì–æ–¥—É–Ω–æ–≤\", \"–ú–æ—Ü–∞—Ä—Ç –∏ –°–∞–ª—å–µ—Ä–∏\"\\n* **–†–∞—Å—Å–∫–∞–∑–æ–≤ –∏ –ø–æ–≤–µ—Å—Ç–µ–π:** \"–ö–∞–ø–∏—Ç–∞–Ω—Å–∫–∞—è –¥–æ—á–∫–∞\", \"–ü–∏–∫–æ–≤–∞—è –¥–∞–º–∞\", \"–î—É–±—Ä–æ–≤—Å–∫–∏–π\"\\n* **–ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –ø–æ–≤–µ—Å—Ç–µ–π:** \"–ê—Ä–∞–ø –ü–µ—Ç—Ä–∞ –í–µ–ª–∏–∫–æ–≥–æ\", \"–ü–æ–≤–µ—Å—Ç–∏ –ë–µ–ª–∫–∏–Ω–∞\"\\n\\n**–í–ª–∏—è–Ω–∏–µ –ü—É—à–∫–∏–Ω–∞:**\\n\\n* **–û—Å–Ω–æ–≤–æ–ø–æ–ª–æ–∂–Ω–∏–∫** —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ä—É—Å—Å–∫–æ–≥–æ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω–æ–≥–æ —è–∑—ã–∫–∞.\\n* **–ï–≥–æ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è** —Å—Ç–∞–ª–∏ –∫–ª–∞—Å—Å–∏–∫–æ–π —Ä—É—Å—Å–∫–æ–π –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã –∏ –ø–µ—Ä–µ–≤–µ–¥–µ–Ω—ã –Ω–∞ –º–Ω–æ–≥–∏–µ —è–∑—ã–∫–∏ –º–∏—Ä–∞.\\n* **–ü—É—à–∫–∏–Ω** –æ–∫–∞–∑–∞–ª –æ–≥—Ä–æ–º–Ω–æ–µ –≤–ª–∏—è–Ω–∏–µ –Ω–∞ —Ä–∞–∑–≤–∏—Ç–∏–µ —Ä—É—Å—Å–∫–æ–π –∫—É–ª—å—Ç—É—Ä—ã –∏ –∏—Å–∫—É—Å—Å—Ç–≤–∞.\\n\\n**–ü–∞–º—è—Ç—å –æ –ü—É—à–∫–∏–Ω–µ:**\\n\\n* **–ï–≥–æ –∏–º—è** –Ω–æ—Å—è—Ç –º—É–∑–µ–∏, —Ç–µ–∞—Ç—Ä—ã, —É–ª–∏—Ü—ã –∏ –≥–æ—Ä–æ–¥–∞ –≤ –†–æ—Å—Å–∏–∏ –∏ –∑–∞ —Ä—É–±–µ–∂–æ–º.\\n* **–î–µ–Ω—å —Ä–æ–∂–¥–µ–Ω–∏—è –ü—É—à–∫–∏–Ω–∞** (6 –∏—é–Ω—è) –æ—Ç–º–µ—á–∞–µ—Ç—Å—è –≤ –†–æ—Å—Å–∏–∏ –∫–∞–∫ **–î–µ–Ω—å —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞**.\\n\\n\\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 417, 'prompt_tokens': 18, 'total_tokens': 435, 'completion_time': 0.758181818, 'prompt_time': 0.002121627, 'queue_time': 0.17830294400000002, 'total_time': 0.760303445}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-31574700-c946-4307-b036-a3fcbdb073ff-0', usage_metadata={'input_tokens': 18, 'output_tokens': 417, 'total_tokens': 435})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemma2_9b_chat.invoke(\"–ö—Ç–æ —Ç–∞–∫–æ–π –ê.–°. –ü—É—à–∫–∏–Ω?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fdc2f0ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello world! üëã \\n\\nIs there anything else I can help you with?\\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 11, 'total_tokens': 30, 'completion_time': 0.034545455, 'prompt_time': 0.001921307, 'queue_time': 0.163365282, 'total_time': 0.036466762}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-e990232e-d7a0-4788-b619-8d989ef85d5d-0', usage_metadata={'input_tokens': 11, 'output_tokens': 19, 'total_tokens': 30})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemma2_9b_chat.invoke(\"hello world\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582c0e5a",
   "metadata": {},
   "source": [
    "The interface is consistent across all chat models and models are typically initialized once at the start up each notebooks. \n",
    "\n",
    "So, you can easily switch between models without changing the downstream code if you have strong preference for another provider.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad0069a",
   "metadata": {},
   "source": [
    "## Search Tools\n",
    "\n",
    "You'll also see [Tavily](https://tavily.com/) in the README, which is a search engine optimized for LLMs and RAG, aimed at efficient, quick, and persistent search results. As mentioned, it's easy to sign up and offers a generous free tier. Some lessons (in Module 4) will use Tavily by default but, of course, other search tools can be used if you want to modify the code for yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "091dff13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "TAVILY_API_KEY:  ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
     ]
    }
   ],
   "source": [
    "_set_env(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52d69da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "tavily_search = TavilySearchResults(max_results=3)\n",
    "search_docs = tavily_search.invoke(\"What is LangGraph?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d06f87e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'What Is LangGraph and How to Use It? - DataCamp',\n",
       "  'url': 'https://www.datacamp.com/tutorial/langgraph-tutorial',\n",
       "  'content': \"Home\\nTutorials\\nArtificial Intelligence\\n\\nLangGraph Tutorial: What Is LangGraph and How to Use It?\\nLangGraph is a library within the LangChain ecosystem that provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured and efficient manner.\\nJun 26, 2024 \\xa0¬∑ 12 min read\\nShare [...] Imagine you're building a complex, multi-agent large language model (LLM) application. It's exciting, but it comes with challenges: managing the state of various agents, coordinating their interactions, and handling errors effectively. This is where LangGraph can help.\\nLangGraph is a library within the LangChain ecosystem designed to tackle these challenges head-on. LangGraph provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured manner.\",\n",
       "  'score': 0.9200085741536458},\n",
       " {'title': 'What is LangGraph? - Analytics Vidhya',\n",
       "  'url': 'https://www.analyticsvidhya.com/blog/2024/07/langgraph-revolutionizing-ai-agent/',\n",
       "  'content': 'To sum up, LangGraph is a major advancement in the development of AI agents. It enables developers to push the limits of what‚Äôs possible with AI agents by eliminating the shortcomings of earlier systems and offering a flexible, graph-based framework for agent construction and execution. LangGraph is positioned to influence the direction of artificial intelligence significantly in the future. [...] LangGraph is a library built on top of Langchain that is designed to facilitate the creation of cyclic graphs for large language model (LLM) ‚Äì based AI agents.\\nIt views agent Objective Points about LangGraph and workflows as cyclic graph topologies, allowing for more variable and nuanced agent behaviors than linear execution models. [...] Frameworks such as LangGraph are becoming increasingly important as AI develops. LangGraph is making the next generation of AI applications possible by offering a versatile and strong framework for developing and overseeing AI agents.',\n",
       "  'score': 0.9126642883246529},\n",
       " {'title': \"Introduction to LangGraph: A Beginner's Guide - Medium\",\n",
       "  'url': 'https://medium.com/@cplog/introduction-to-langgraph-a-beginners-guide-14f9be027141',\n",
       "  'content': 'LangGraph is a powerful tool for building stateful, multi-actor applications with Large Language Models (LLMs). It extends the LangChain library, allowing you to coordinate multiple chains (or actors) across multiple steps of computation in a cyclic manner. In this article, we‚Äôll introduce LangGraph, walk you through its basic concepts, and share some insights and common points of confusion for beginners.\\nWhat is LangGraph? [...] LangGraph is a library built on top of LangChain, designed to add cyclic computational capabilities to your LLM applications. While LangChain allows you to define chains of computation (Directed Acyclic Graphs or DAGs), LangGraph introduces the ability to add cycles, enabling more complex, agent-like behaviors where you can call an LLM in a loop, asking it what action to take next.\\nKey Concepts\\nA Simple Example [...] LangGraph is a versatile tool for building complex, stateful applications with LLMs. By understanding its core concepts and working through simple examples, beginners can start to leverage its power for their projects. Remember to pay attention to state management, conditional edges, and ensuring there are no dead-end nodes in your graph. Happy coding!\\n--\\n--\\n9\\nWritten by CPlog',\n",
       "  'score': 0.9037418992534721}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafd7d5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lc-academy-env",
   "language": "python",
   "name": "lc-academy-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
